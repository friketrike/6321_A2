\documentclass{article}

% \usepackage[margin=0.75in]{geometry}

\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{epstopdf} 
\usepackage{caption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{listings}
\usepackage{float}

\lstset{language=Matlab}

\usepackage{stackengine}
\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}


\titleformat{\section}
  {\normalfont\Large\bfseries}   % The style of the section title
  {}                             % a prefix
  {0pt}                          % How much space exists between the prefix and the title
  {Question \thesection:\quad}    % How the section is represented

% Starred variant
\titleformat{name=\section,numberless}
  {\normalfont\Large\bfseries}
  {}
  {0pt}
  {}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\begin{document}
\title{Comp 6321 - Machine Learning - Assignment 2}
\author{Federico O'Reilly Regueiro}
\date{October 18th, 2016}
\maketitle

%------------------------ Q1 ------------------------%
\section{} 
%------------------------  a ------------------------%
\subsection{Partition the data into training / testing, 90 to 10 and perform and plot $L2$ regularization}
The data was partitioned pseudo-randomly\footnote{I have included the permutation indexes yielded by 
matlab for the instance of the 90 / 10 partition from which the plots and values were drawn. Suffice it to 
uncomment two lines and comment-out two others in order to partition the data randomly, 
yet similar results (at different scales of $\lambda$) can be observed.}.

\begin{lstlisting}
phi = load('hw2x.dat');
phi = [phi, ones(size(phi,1),1)];
y = load('hw2y.dat');

% Partition the data randomly.
idxs = randperm(size(phi, 1));
idx_train = idxs(1:89);
idx_test = idxs(90:99);

phi_train = phi(idx_train, :);
y_train = y(idx_train);
phi_test = phi(idx_test, :);
y_test = y(idx_test);
\end{lstlisting}

Next, a range of lambdas was chosen, going from 0 to almost 125000 in order to get a good sense of the trend of both the error 
and the coefficients. The former was plotted for a range of small values as well as along the whole set of lambdas %TODO check if this is worth it, then implement if so
 for which the $RMS$ error was calculated. Two plots were done in order to allow us to see the behaviour of the test and training errors
  for lower values of $\lambda$ as well as the overall trend of the $RMS$ the resulting plot can be seen in Figure \ref{fig:rms-w-lambda-l2}.

\begin{lstlisting}
lambdas = 0:0.1:50;
lambdas = lambdas .^ 3;

w = zeros(length(lambdas), size(phi,2));

for lambda = lambdas
  idx = lambda == lambdas;
  
  % Train the model
  w(idx, :) = pinv(phi_train' * phi_train ...
                + (lambda * eye(size(phi_train, 2)))) ...
                * (phi_train' * y_train);
                
  h_phi_train(:, idx) = phi_train * w(idx, :)';
  j_h_train(idx) = rms(h_phi_train(:, idx) - y_train);
  
  % Now compare to the test
  h_phi_test(:, idx) = phi_test * w(idx, :)';
  j_h_test(idx) = rms(h_phi_test(:, idx) - y_test);        
end
\end{lstlisting}

\begin{figure}[H]
\begin{center}
	\includegraphics[width=4in, trim=1in 3in 1in 3in]{RMS_w_lambda_L2}
\caption{Plot of the RMS training and testing error as well as the coefficients over a wide range of $\lambda$.}\label{fig:rms-w-lambda-l2}%      only if needed  
\end{center}
\end{figure}
On the top-left plot of figure \ref{fig:rms-w-lambda-l2} can observe that as expected, the test error goes down as $\lambda$ grows. Howerver, as $\lambda$ continues to increase, the test error closely follows the training error as they both grow since the restrictions on the coefficients make for a worse fit at a certain point.

%------------------------------------- b ------------------------------------------ %
\subsection{Use the \texttt{quadprog} function in Matlab for $L1$ regularization}
In order to use \texttt{quadprog} for regularization, we must first find the Hessian matrix \texttt{H} as well as other parameters
\texttt{f}, \texttt{A}, \texttt{b} in the specific format that Matlab requires.

We recall that for $L1$ regularization the expression we must minimize is:
\begin{equation}
	\arg \min\limits_{w}\frac{1}{2}(\bm{\Phi w - y})^T(\bm{\Phi w - y}) + \frac{\lambda}{2}\sum\limits_{k=0}^{K-1}|w_k|\\
\end{equation}
Which is equivalent to finding:
\begin{equation}
    \begin{aligned}
        \arg \min\limits_{w}(&\bm{\Phi w - y})^T(\bm{\Phi w - y})\\
                             & \sum\limits_{k=0}^{K-1}|w_k| \leq \eta
    \end{aligned}
\end{equation}
And expands to:
\begin{equation}
    \begin{aligned}
        \arg \min\limits_{w}&\bm{w}^T\bm{\Phi}^T\bm{\Phi w} - 2y^T\bm{\Phi w} \\
                             & \sum\limits_{k=0}^{K-1}|w_k| \leq \eta    
    \end{aligned}
\end{equation}
And for which we can remove the constant term $\bm{y}^T\bm{y}$, yielding:
\begin{equation}
    \begin{aligned}
        \arg \min\limits_{w}&\bm{w}^T\bm{\Phi}^T\bm{\Phi w - 2y}^T\bm{\Phi w}^T \\
                             & \sum\limits_{k=0}^{K-1}|w_k| \leq \eta    
    \end{aligned}
\end{equation}
Matlab's \texttt{quadprog(H, f, A, b)} function, gives the optimal $\bm{x}$ corresponding to the expression 
$\arg\min\limits_{x}  \frac{1}{2}\bm{x}^T\bm{Hx} + \bm{f}^T\bm{x}$,  
subject to constraints 
$\bm{Ax} \leq \bm{b}$. 
We can thus take \texttt{H :=} $2\bm{\Phi}^T\bm{\Phi}$, 
then \texttt{f :=} $-2\bm{y}^T\bm{\Phi}$, 
\texttt{A :=}$\bm{P}$, where for a system with $n$  variables, 
$\eta \bm{P}$ is the matrix with $2^n$ permutations of $[b_1, b_2, \ldots b_n], b \in \{-1, 1\}$ 
and lastly \texttt{b :=} $c\bm{\stackrel{\rightarrow}{1}}$, 
where$\bm{\stackrel{\rightarrow}{1}}$ is an all-one vector of length $2^n$ that places an upper bound, 
$\eta$, to the expression 
$\sum_{k=0}^{K-1}|w_k|$, such that $\sum_{k=0}^{K-1}|w_k| \leq \eta$.
We note that $\eta$ is roughly equivalent to $\frac{1}{\lambda}$ which we use so that we may compare the 
effects of $L1$ and $L2$ regularizations on a similar range of values.

Thus we end up with the following code:
\begin{lstlisting}
etas = 1./lambdas;
for eta = etas
  idx = eta == etas;
  w_quad(idx,:) = quadprog(2*(phi_train'*phi_train), ...
                    -2*(phi_train'*y_train), ...
                        [ 1, 1, 1, 1;   1, 1,-1, 1; ...
                          1,-1, 1, 1;   1,-1,-1, 1; ...
                         -1, 1, 1, 1;  -1, 1,-1, 1; ...
                          1,-1, 1, 1;  -1,-1,-1, 1; ... 
                          1, 1, 1, -1;   1, 1,-1, -1; ...
                          1,-1, 1, -1;   1,-1,-1, -1; ...
                         -1, 1, 1, -1;  -1, 1,-1, -1; ...
                          1,-1, 1, -1;  -1,-1,-1, -1], ... 
                     eta * [1; 1; 1; 1; 1; 1; 1; 1]);

  h_phi_train_quad(:, idx) = phi_train * w_quad(idx, :)';
  j_h_train_quad(idx) = rms(h_phi_train_quad(:, idx) - y_train); 

  % Now validate
  h_phi_test_quad(:, idx) = phi_test * w_quad(idx, :)';
  j_h_test_quad(idx) = rms(h_phi_test_quad(:, idx) - y_test);
end
\end{lstlisting}

% ----------------------------- c ---------------------------
\subsection{Plot $L1$ $RMS$ and coefficients, $w$ against $\lambda$ and comment}
Although it is not exactly equivalent, we shall simplify the comparison of $L1$ and $L2$ 
throughout this section by using $\lambda \approx \frac{1}{\eta}$ as it gives a clearer idea.
In Figure \ref{fig:rms-lambda-l1}, we can again notice how the test error slightly decreases for 
the lowest values of $\approx \lambda$ and then monotonically increases as the coefficients are 
all forced towards 0. We also note that $L1$ yields a lower minimum error (1.0098) than $L2$ 
regularization (1.0257).

\begin{figure}[H]
\begin{center}
	\includegraphics[width=4in, trim=1in 3in 1in 3in]{RMS_lambda_L1}
    \caption{The RMS training and testing error for a wide range of $\frac{1}{\eta} \approx \lambda$.}\label{fig:rms-lambda-l1}
\end{center}
\end{figure}

By the same token, in figure \ref{fig:w-lambda-l1} we can see how both $w_2$ and $w_3$ sharply decrease to 0 when
$\approx \lambda=2$ and the model relies solely on $w_1$\footnote{$w_4$ is the bias 
term, so we can't really say the model relies on it a it is not an input.}
which then decreases gradually, as opposed to figure \ref{fig:rms-w-lambda-l2}, where we can observe how all coefficients
approach 0 at a similar rate during $L2$ regularization. Conversely, as is expected we can see that both
errors and coefficients are equal between $L1$ and $L2$ regularization when $\lambda =0$.

This particular data-set would lead to believe that the data was generated mainly by some function $f(w_1) + \epsilon$. This hypothesis
is supported by the following output:
\begin{lstlisting}
corr(y, phi(:,1:3))
ans =
  -0.848189  -0.017339  -0.024943
\end{lstlisting}
which reveals that the correlation between $\Phi_{:,1}$ and $y$ is much larger than between other columns of $\Phi$ and $y$.

\begin{figure}[H]
\begin{center}
	\includegraphics[width=4in, trim=1in 3in 1in 3in]{w_lambda_L1}
\caption{The RMS training and testing error for a wide range of $\frac{1}{\eta} \approx \lambda$.}\label{fig:w-lambda-l1}
\end{center}
\end{figure}


% -------------------------------------------- Q 2 ------------------------------------------------
\section{Dealing with missing data, fill in $x_{i,n}$ with class-conditional means?}
First, we write $\mu_{c,i}$ to represent $E(x_i| y = c)$ and we assume independence between features. Then,
since our classifier is Gaussian, we know that $P(x|y=1)$ and $P(x|y=0)$ are modelled as follows:

\begin{equation}
P(x|y = c) = \frac{1}{\sqrt{2\pi |\bm{\Sigma}|}}e^{-\frac{1}{2} (\bm{x}-\bm{\mu_c})^T \bm{\Sigma}^{-1} (\bm{x}-\bm{\mu_c}), \quad c \in \{0,1\}} 
\end{equation}
We turn our attention to the numerator of the exponent, which can also be written in the following manner:
\begin{equation}
\sum\limits_{i=1}^{n}[(x_i-\mu_{c,i}\sum\limits_{j=1}^{n}({\Sigma^{-1}}_{i,j} (x_j-\mu_{c,j}) ]
\end{equation}
For the value of a given feature $n$, replaced by its class-conditional mean, $\mu_{c,n}$, this expression becomes $0$; we further develop this by analyzing the log-odds:
\begin{equation}
log\frac{P(y=1|x)}{P(y=0|x)} = log\frac{P(y=1)}{P(y=0)}+ log\frac{\frac{1}{\sqrt{2\pi |\bm{\Sigma}|}}e^{-\frac{1}{2} (\bm{x}-\bm{\mu_1})^T \bm{\Sigma}^{-1} (\bm{x}-\bm{\mu_1})}}
{\frac{1}{\sqrt{2\pi |\bm{\Sigma}|}}e^{-\frac{1}{2} (\bm{x}-\bm{\mu_0})^T \bm{\Sigma}^{-1} (\bm{x}-\bm{\mu_0})} }
\end{equation}
Since the matrix sigma is shared, we can write:
\begin{equation}
log\frac{P(y=1|x)}{P(y=0|x)} = log\frac{P(y=1)}{P(y=0)}+
 log\frac{
-\frac{1}{2} (\bm{x}-\bm{\mu_1})^T \bm{\Sigma}^{-1} (\bm{x}-\bm{\mu_1})
}
{
-\frac{1}{2} (\bm{x}-\bm{\mu_0})^T \bm{\Sigma}^{-1} (\bm{x}-\bm{\mu_0})
}
\end{equation}
Then we expand the exponent:
\begin{equation}
log\frac{P(y=1|x)}{P(y=0|x)} = log\frac{P(y=1)}{P(y=0)}+
 log\frac{
-\frac{1}{2}\sum\limits_{i=1}^{n}[(x_i-\mu_{c,i}\sum\limits_{j=1}^{n}({\Sigma^{-1}}_{i,j} (x_j-\mu_{c,j}) ])
 }
{
-\frac{1}{2}\sum\limits_{i=1}^{n}[(x_i-\mu_{c,i}\sum\limits_{j=1}^{n}({\Sigma^{-1}}_{i,j} (x_j-\mu_{c,j}) ])
}
\end{equation}
Where we can clearly see that the contribution of $x_n$ does not change the ratio of the log-odds given by all other features, since we have choosen $x_n = \mu_{c,n}$ for $c \in \{0,1\}$ where $\mu_{c,n}$ is the class-conditional means for class $c$

%------------------------------------------- Q 3 ----------------------------------------------------
\section{Naive Bayes assumption, suppose a feature gets repeated in the model}

%------------------------------------------ Q 4 ----------------------------------------------------
\section{If only anything like this had been touched-upon in class...}

%------------------------------------------ Q 4 ----------------------------------------------------
\section{Implementation of Logistic Regression and Gaussian Naive-Bayes}
\subsection*{A little bit about our data} % label something

\begin{verbatim}
-  The means
	- near-coincident class-means, and class-variances - see FIG below
	- nicely centered around zero...
- The validity of the naive assumption
	- example of highly correlated pairs of features
		-FIG `5-corr-dist' [2 vs 5 no corr, 11 vs 14, high corr (banana); 2 similar mu_c, slightly diff sigma_c, 11. similar sigma_c, slightly differing mu_c]
	- resulting variance-covariance matrix of X
		-FIG 5-cov-x notice the high covariances for features in the corners off the diag...
	
- Scaling

\end{verbatim}

\subsection{Logistic rergession}
Yada yada

\subsection{Implementation of GNB classifier}


\end{document}
